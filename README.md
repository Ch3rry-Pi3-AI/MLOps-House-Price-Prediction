# **Model Inference Stage**

This branch extends the **MLOps House Price Prediction** project by implementing the **model inference pipeline**.
It introduces a production-ready inference stack composed of:

* **FastAPI** ‚Äì serves predictions from the trained model.
* **Streamlit** ‚Äì provides an intuitive web interface for real-time user interaction.
* **Docker & Docker Compose** ‚Äì containerise and orchestrate both services for seamless execution.

This stage operationalises the trained model and preprocessor into a **live, queryable inference service** with an accompanying UI for end-users.



## **Project Structure**

```
mlops-house-price-prediction/
‚îú‚îÄ‚îÄ .venv/
‚îú‚îÄ‚îÄ .github/
‚îú‚îÄ‚îÄ data/
‚îú‚îÄ‚îÄ models/
‚îú‚îÄ‚îÄ notebooks/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ api/                                # üß† FastAPI inference service
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ inference.py                    #   Loads model + preprocessor, defines predict()
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py                         #   FastAPI entrypoint and routing
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schemas.py                      #   Pydantic request/response models
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt                #   FastAPI + Uvicorn dependencies
‚îÇ   ‚îú‚îÄ‚îÄ data/
‚îú‚îÄ‚îÄ streamlit_app/                          # üé® Streamlit user interface
‚îÇ   ‚îú‚îÄ‚îÄ app.py                              #   Web UI calling the FastAPI backend
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt                    #   Streamlit + Requests dependencies
‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile                          #   Streamlit container definition
‚îú‚îÄ‚îÄ img/                                    # üñºÔ∏è Media assets (GIFs, screenshots, etc.)
‚îÇ   ‚îî‚îÄ‚îÄ streamlit_app.gif                   #   Demonstration of the inference app
‚îú‚îÄ‚îÄ Dockerfile                              # FastAPI container definition
‚îú‚îÄ‚îÄ docker-compose.yaml                     # Multi-service orchestration (FastAPI + Streamlit)
‚îú‚îÄ‚îÄ tasks.py
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ uv.lock
```

> Note: Any `.venv/` directory remains ignored and should not be committed.



## **Inference Overview**

### üß† `src/api/` ‚Äî FastAPI Inference Service

* Loads the trained model (`house_price_model.pkl`) and preprocessor.
* Exposes two endpoints:

  * **`/health`** ‚Äî health check to confirm the API is live.
  * **`/predict`** ‚Äî accepts JSON input conforming to `HousePredictionRequest` and returns the predicted house price.
* Runs via **Uvicorn** on port `8000` inside its container.



### üé® `streamlit_app/` ‚Äî Streamlit Frontend

* Provides an interactive web interface for inputting house attributes.
* Sends requests to the FastAPI backend using the `API_URL` environment variable (e.g., `http://fastapi:8000`).
* Displays:

  * Predicted price
  * Model used
  * Top three most influential factors
  * Prediction latency (in milliseconds)
* Runs via **Streamlit** on port `8501` inside its container.

<p align="center">
  <img src="img/streamlit_app.gif" alt="Streamlit App Demo" width="800"/>
</p>



## **Containerisation & Orchestration**

### üß© Dockerfiles

* **Root `Dockerfile`** ‚Üí builds the FastAPI inference service.
* **`streamlit_app/Dockerfile`** ‚Üí builds the Streamlit interface.

### ‚öôÔ∏è `docker-compose.yaml`

Defines and links both services:

```yaml
services:
  fastapi:
    build: .
    ports: ["8000:8000"]

  streamlit:
    build: ./streamlit_app
    ports: ["8501:8501"]
    environment:
      API_URL: http://fastapi:8000
    depends_on:
      - fastapi
```

Docker Compose automatically networks the two containers, so the Streamlit frontend communicates with the FastAPI backend via the hostname `fastapi`.



## **Building and Running the Inference Stack**

### üèóÔ∏è Build both images

```bash
docker compose build
```

### üöÄ Launch the stack

```bash
docker compose up
# or detached mode:
docker compose up -d
```

### üåê Access the services

| Service       | URL                                                      |
| - | -- |
| **FastAPI**   | [http://localhost:8000/docs](http://localhost:8000/docs) |
| **Streamlit** | [http://localhost:8501](http://localhost:8501)           |



## **Testing the FastAPI Endpoint**

### ‚úÖ Health Check

```bash
curl http://localhost:8000/health
```

### üß† Example Prediction Request

```bash
curl -X POST "http://localhost:8000/predict" \
     -H "Content-Type: application/json" \
     -d '{"sqft":2000,"bedrooms":3,"bathrooms":2,"year_built":2010,"condition":"Good"}'
```

Expected response:

```json
{"predicted_price": 354820.45, "currency": "USD"}
```



## **Publishing to Docker Hub**

### 1Ô∏è‚É£ Log in

```bash
docker login
# username: ch3rrypi3
```

### 2Ô∏è‚É£ Push the images

```bash
docker push ch3rrypi3/fastapi:inference
docker push ch3rrypi3/streamlit:inference
```

### 3Ô∏è‚É£ Verify upload

Check your repositories at
üëâ [https://hub.docker.com/repositories/ch3rrypi3](https://hub.docker.com/repositories/ch3rrypi3)



## **Useful Docker Commands**

| Purpose                                 | Command                                         |
|  | -- |
| List running containers                 | `docker ps`                                     |
| List all containers (including stopped) | `docker ps -a`                                  |
| Stop containers                         | `docker compose down`                           |
| Remove all containers, images, networks | `docker system prune -a`                        |
| View image list                         | `docker images`                                 |
| Tail logs live                          | `docker compose logs -f`                        |
| Build a single image                    | `docker build -t ch3rrypi3/fastapi:inference .` |
| Push image to Docker Hub                | `docker push ch3rrypi3/fastapi:inference`       |



## ‚úÖ Summary

This **Inference Stage** completes the MLOps House Price Prediction pipeline by providing:

* A **FastAPI** backend for real-time model inference.
* A **Streamlit** web app for interactive visualisation and prediction.
* Full **Docker and Compose** orchestration for portability and reproducibility.
* Optional **Docker Hub** publishing for versioned distribution.

With this stage, the project delivers a **fully containerised, end-to-end ML inference system** ready for local or cloud deployment. üöÄ

