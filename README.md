# **Data Preprocessing Stage**

This branch extends the **MLOps House Price Prediction** project by implementing the **data preprocessing pipeline**.
It introduces a modular structure under `src/data/` for loading, cleaning, schema validation, and outlier handling, alongside a full test suite, automation via `invoke`, and GitHub Actions for CI/CD.

A new **data preprocessing notebook** (`notebooks/01_data_preprocessing.ipynb`) has also been added.
This represents the **data scientist's exploratory workflow**, where preprocessing logic is first developed interactively. Once validated, it is handed off to an **ML engineer**, who modularises the code under `src/data/` and integrates it into the CI/CD system.

The pipeline can now take raw data (`data/raw/`) and output cleaned data (`data/processed/cleaned_house_data.csv`) with missing value imputation and configurable outlier handling.

---

## **Project Structure**

```
mlops-house-price-prediction/
â”œâ”€â”€ .venv/
â”œâ”€â”€ .github/                        # ðŸš€ NEW: GitHub Actions configuration
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci.yml                  # ðŸš€ NEW: CI pipeline (tests, linting, etc.)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â”‚       â””â”€â”€ cleaned_house_data.csv  # ðŸš€ NEW: Example processed dataset
â”œâ”€â”€ deployment/
â”‚   â”œâ”€â”€ kubernetes/
â”‚   â””â”€â”€ mlflow/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ trained/
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ 01_data_preprocessing.ipynb # ðŸš€ NEW: Data scientist's preprocessing notebook
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ data/                       # ðŸš€ NEW: Preprocessing modules
â”‚   â”‚   â”œâ”€â”€ cleaning.py             # ðŸš€ NEW: Missing value handling
â”‚   â”‚   â”œâ”€â”€ config.py               # ðŸš€ NEW: Config (dataclass, YAML loader)
â”‚   â”‚   â”œâ”€â”€ io.py                   # ðŸš€ NEW: CSV/Parquet load & save helpers
â”‚   â”‚   â”œâ”€â”€ outliers.py             # ðŸš€ NEW: Outlier detection & policies
â”‚   â”‚   â”œâ”€â”€ processor.py            # ðŸš€ NEW: Orchestrator for preprocessing
â”‚   â”‚   â”œâ”€â”€ schema.py               # ðŸš€ NEW: Schema/column validation
â”‚   â”‚   â””â”€â”€ cli.py                  # ðŸš€ NEW: Command-line entrypoint for preprocessing
â”‚   â”œâ”€â”€ features/
â”‚   â””â”€â”€ models/
â”œâ”€â”€ streamlit_app/
â”œâ”€â”€ tests/                          # ðŸš€ NEW: Test suite
â”‚   â”œâ”€â”€ conftest.py                 # ðŸš€ NEW: Shared fixtures + sys.path shim
â”‚   â””â”€â”€ data/
â”‚       â”œâ”€â”€ test_cleaning.py        # ðŸš€ NEW: Unit tests for cleaning
â”‚       â”œâ”€â”€ test_io.py              # ðŸš€ NEW: Unit tests for IO
â”‚       â”œâ”€â”€ test_outliers.py        # ðŸš€ NEW: Unit tests for outliers
â”‚       â”œâ”€â”€ test_processor_integration.py # ðŸš€ NEW: End-to-end integration test
â”‚       â””â”€â”€ test_schema.py          # ðŸš€ NEW: Unit tests for schema checks
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .python-version
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ tasks.py                        # ðŸš€ NEW: Invoke task runner (tests, lint, preprocess, serve, etc.)
â””â”€â”€ uv.lock
```

> Note: Any `.venv/` folder is ignored and should not be committed.

---

## **Development Environment**

This project uses [uv](https://github.com/astral-sh/uv) for Python environment and dependency management.
You will also need Docker for running MLflow locally and Kubernetes manifests for deployment.

### 1) Prerequisites

Install the following tools:

* [Python 3.13](https://www.python.org/downloads/)
* [Git](https://git-scm.com/)
* [Visual Studio Code](https://code.visualstudio.com/) (or another editor)
* [uv â€“ Python package and environment manager](https://github.com/astral-sh/uv)
* [Docker Desktop](https://www.docker.com/products/docker-desktop)

### 2) Clone the repository

```bash
git clone https://github.com/<your-username>/MLOps-House-Price-Prediction.git
cd MLOps-House-Price-Prediction
```

### 3) Create a virtual environment

```bash
uv venv --python python3.13
```

Activate it:

```bash
# On Linux / macOS
source .venv/bin/activate

# On Windows (PowerShell)
.venv\Scripts\Activate.ps1

# On Windows (Git Bash)
source .venv/Scripts/activate
```

### 4) Install dependencies

```bash
uv pip install -r requirements.txt
```

### 5) Deactivate when done

```bash
deactivate
```

---

## **Running Preprocessing**

You can now run the preprocessing pipeline in four ways:

### 1. Notebook Execution (Data Scientist Workflow)

Run and explore `notebooks/01_data_preprocessing.ipynb` interactively.
This is where preprocessing logic is first designed and validated.

### 2. Direct Python Execution (ML Engineer Workflow)

```bash
python -m src.data.processor
```

Runs the pipeline with defaults:
`data/raw/house_data.csv` â†’ `data/processed/cleaned_house_data.csv`.

### 3. Command-Line Interface (CLI)

```bash
python -m src.data.cli --in data/raw/house_data.csv --out data/processed/cleaned_house_data.csv --policy filter
```

Supports YAML configs and runtime overrides.

### 4. Using Invoke Tasks

```bash
invoke preprocess
invoke preprocess --policy=clip --target=price --iqr=2.0
invoke test
invoke cov
```

---

## **Continuous Integration (CI/CD)**

This stage also introduces a **GitHub Actions workflow** (`.github/workflows/ci.yml`) for automated testing and linting.

* âœ… Runs `pytest` on every push and pull request
* âœ… Ensures code quality via `ruff` and optional `black` formatting
* âœ… Provides quick feedback in GitHub before merging

---

âœ… With this stage complete, the project now has:

* A **data scientist's notebook** for preprocessing exploration (`01_data_preprocessing.ipynb`)
* A fully modular preprocessing pipeline in `src/data/`
* Automated tests and coverage
* Developer productivity tasks via Invoke
* Continuous integration checks via GitHub Actions

This clearly separates the **exploration stage** (notebooks) from the **engineering stage** (modules + CI/CD), setting the foundation for **exploratory data analysis (EDA)** in the next branch.