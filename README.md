# **Data Preprocessing Stage**

This branch extends the **MLOps House Price Prediction** project by implementing the **data preprocessing pipeline**.
It introduces a modular structure under `src/data/` for loading, cleaning, and handling outliers, alongside a full test suite and automation via `invoke`.

The pipeline can now take raw data (`data/raw/`) and output cleaned data (`data/processed/cleaned_house_data.csv`) with missing value imputation and configurable outlier handling.



## **Project Structure**

```
mlops-house-price-prediction/
â”œâ”€â”€ .venv/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â”‚       â””â”€â”€ cleaned_house_data.csv  # ðŸš€ NEW: Example processed dataset
â”œâ”€â”€ deployment/
â”‚   â”œâ”€â”€ kubernetes/
â”‚   â””â”€â”€ mlflow/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ trained/
â”œâ”€â”€ notebooks/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ data/                       # ðŸš€ NEW: Preprocessing modules
â”‚   â”‚   â”œâ”€â”€ cleaning.py             # ðŸš€ NEW: Missing value handling
â”‚   â”‚   â”œâ”€â”€ config.py               # ðŸš€ NEW: Config (dataclass, YAML loader)
â”‚   â”‚   â”œâ”€â”€ io.py                   # ðŸš€ NEW: CSV/Parquet load & save helpers
â”‚   â”‚   â”œâ”€â”€ outliers.py             # ðŸš€ NEW: Outlier detection & policies
â”‚   â”‚   â”œâ”€â”€ processor.py            # ðŸš€ NEW: Orchestrator for preprocessing
â”‚   â”‚   â”œâ”€â”€ schema.py               # ðŸš€ NEW: Schema/column validation
â”‚   â”‚   â””â”€â”€ cli.py                  # ðŸš€ NEW: Command-line entrypoint for preprocessing
â”‚   â”œâ”€â”€ features/
â”‚   â””â”€â”€ models/
â”œâ”€â”€ streamlit_app/
â”œâ”€â”€ tests/                          # ðŸš€ NEW: Test suite
â”‚   â”œâ”€â”€ conftest.py                 # ðŸš€ NEW: Shared fixtures + sys.path shim
â”‚   â””â”€â”€ data/
â”‚       â”œâ”€â”€ test_cleaning.py        # ðŸš€ NEW: Unit tests for cleaning
â”‚       â”œâ”€â”€ test_io.py              # ðŸš€ NEW: Unit tests for IO
â”‚       â”œâ”€â”€ test_outliers.py        # ðŸš€ NEW: Unit tests for outliers
â”‚       â”œâ”€â”€ test_processor_integration.py # ðŸš€ NEW: End-to-end integration test
â”‚       â””â”€â”€ test_schema.py          # ðŸš€ NEW: Unit tests for schema checks
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .python-version
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ tasks.py                        # ðŸš€ NEW: Invoke task runner (tests, lint, preprocess, serve, etc.)
â””â”€â”€ uv.lock
```

> Note: Any `.venv/` folder is ignored and should not be committed.



## **Development Environment**

This project uses [uv](https://github.com/astral-sh/uv) for Python environment and dependency management.
You will also need Docker for running MLflow locally and Kubernetes manifests for deployment.

### 1) Prerequisites

Install the following tools:

* [Python 3.13](https://www.python.org/downloads/)
* [Git](https://git-scm.com/)
* [Visual Studio Code](https://code.visualstudio.com/) (or another editor)
* [uv â€“ Python package and environment manager](https://github.com/astral-sh/uv)
* [Docker Desktop](https://www.docker.com/products/docker-desktop)

### 2) Clone the repository

```bash
git clone https://github.com/<your-username>/MLOps-House-Price-Prediction.git
cd MLOps-House-Price-Prediction
```

### 3) Create a virtual environment

```bash
uv venv --python python3.13
```

Activate it:

```bash
# On Linux / macOS
source .venv/bin/activate

# On Windows (PowerShell)
.venv\Scripts\Activate.ps1

# On Windows (Git Bash)
source .venv/Scripts/activate
```

### 4) Install dependencies

```bash
uv pip install -r requirements.txt
```

### 5) Deactivate when done

```bash
deactivate
```



## **Running Preprocessing**

You can now run the preprocessing pipeline in two ways:

### 1. Direct Python Execution

```bash
python -m src.data.processor
```

This will load the raw dataset from `data/raw/house_data.csv` and write the cleaned version to `data/processed/cleaned_house_data.csv`.



### 2. Using Invoke Tasks

The project includes an `invoke`-based task runner (`tasks.py`) with common commands:

```bash
# run tests
invoke test

# run a subset
invoke test -k cleaning

# run with coverage
invoke cov

# optional: if you install black/ruff
invoke fmt
invoke lint

# clean caches
invoke clean

# Preprocess with defaults
invoke preprocess

# Preprocess with clipping and a different target
invoke preprocess --policy=clip --target=SalePrice --iqr=2.0

# Create common dirs
invoke ensure-dirs
```



âœ… With this stage complete, the project now has a fully modular preprocessing pipeline, automated testing, and developer productivity commands via Invoke. This ensures data consistency and makes the foundation ready for **feature engineering** and **model training** in the next stage.

